<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quantized Modules &mdash; AI Model Efficiency Toolkit Documentation: ver torch_1.32.0</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/style.css" />
      <link rel="stylesheet" href="../_static/style.css" type="text/css" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Quantizers" href="quantizer.html" />
    <link rel="prev" title="AIMET AdaRound" href="../user_guide/adaround.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

    
    
    <a href="index.html" class="icon icon-home">
    AI Model Efficiency Toolkit
      <img src="../_static/brain_logo.png" class="logo" alt="Logo"/>
    </a>
      <div class="version">
        torch_1.32.0
      </div>

  <div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="Versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      AIMET Variant: <span class="rst-current-version-name"> PyTorch </span>
      <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
      <dl>
        <dt>Documentation Versions</dt>
        
          <dd><a href="https://quic.github.io/aimet-pages/releases/latest/user_guide/index.html">Universal</a></dd>
        
          <dd><a href="https://quic.github.io/aimet-pages/releases/latest/torch_v2/torch_docs/index.html">PyTorch</a></dd>
        
      </dl>
    </div>
  </div>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/quickstart.html">Quickstart Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/ptq.html">Post-Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Feature Descriptions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/adaround.html"> Adaptive Rounding (AdaRound)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AIMET PyTorch API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quantized Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantizer.html">Quantizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoding_analyzer.html">Encoding Analyzers</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/nn.fake_quantization_mixin.html">nn.FakeQuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/nn.quantization_mixin.html">nn.QuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/q.quantized_tensor.html">quantization.QuantizedTensor</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AI Model Efficiency Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Quantized Modules</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/torch_docs/quantized_modules.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul><div class="rst-breadcrumbs-buttons" role="navigation" aria-label="Sequential page navigation">
        <a href="../user_guide/adaround.html" class="btn btn-neutral float-left" title="AIMET AdaRound" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="quantizer.html" class="btn btn-neutral float-right" title="Quantizers" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
  </div>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="admonition warning" id="api-torch-quantized-modules">
<p class="admonition-title">Warning</p>
<p>This feature is under heavy development and API changes may occur without notice in future verions.</p>
</div>
<div class="section" id="quantized-modules">
<h1>Quantized Modules<a class="headerlink" href="#quantized-modules" title="Permalink to this heading"></a></h1>
<p>To simulate the effects of running networks at an reduced (or integer) bitwidth, AIMET provides quantized versions of
standard torch.nn.Modules. These quantized modules serve as drop-in replacements for their PyTorch counterparts, but can
hold input, output, and parameter <a class="reference internal" href="quantizer.html#api-torch-quantizers"><span class="std std-ref">quantizers</span></a> to perform quantization operations during the
module’s forward pass and compute quantization encodings.</p>
<p>A quantized module inherits both from an AIMET-defined quantization mixin type as well as a native pytorch <cite>nn.Module</cite> type. The
exact behavior and capabilities of the quantized module are determined by which type of quantization mixin it inherits from.</p>
<p>AIMET defines two types of quantization mixin:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="api/nn.fake_quantization_mixin.html#api-torch-fake-quantization-mixin"><span class="std std-ref">FakeQuantizationMixin</span></a>: Simulates quantization by performing quantize-dequantize
operations on tensors and calling into native pytorch floating-point operations</p></li>
<li><p><a class="reference internal" href="api/nn.quantization_mixin.html#api-torch-quantization-mixin"><span class="std std-ref">QuantizationMixin</span></a>: Allows the user to register a custom kernel to perform
a quantized forward pass and dequantizes the output. If no kernel is registered, the module will perform fake-quantization.</p></li>
</ul>
</div></blockquote>
<p>The functionality and state of a <a class="reference internal" href="api/nn.quantization_mixin.html#api-torch-quantization-mixin"><span class="std std-ref">QuantizationMixin</span></a> is a superset of that of a <a class="reference internal" href="api/nn.fake_quantization_mixin.html#api-torch-fake-quantization-mixin"><span class="std std-ref">FakeQuantizationMixin</span></a>, meaning that
if one does not intend to register a custom kernel, it does not matter which mixin a module is inherited from. AIMET provides
extensive coverage of <a class="reference internal" href="api/nn.fake_quantization_mixin.html#api-torch-fake-quantization-mixin"><span class="std std-ref">FakeQuantizationMixin</span></a> for <cite>torch.nn.Module</cite> layer types, and more limited coverage for
<a class="reference internal" href="api/nn.quantization_mixin.html#api-torch-quantization-mixin"><span class="std std-ref">QuantizationMixin</span></a> layers.</p>
<div class="section" id="top-level-api">
<h2>Top-level API<a class="headerlink" href="#top-level-api" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.base.BaseQuantizationMixin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">aimet_torch.v2.nn.base.</span></span><span class="sig-name descname"><span class="pre">BaseQuantizationMixin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/nn/base.html#BaseQuantizationMixin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#aimet_torch.v2.nn.base.BaseQuantizationMixin" title="Permalink to this definition"></a></dt>
<dd><p>Mixin that implements quantization on top of regular pytorch modules.</p>
<dl class="py method">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.base.BaseQuantizationMixin.__quant_init__">
<span class="sig-name descname"><span class="pre">__quant_init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/nn/base.html#BaseQuantizationMixin.__quant_init__"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#aimet_torch.v2.nn.base.BaseQuantizationMixin.__quant_init__" title="Permalink to this definition"></a></dt>
<dd><p>Initializer for quantized module. This method will be invoked right after __init__.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.base.BaseQuantizationMixin.compute_encodings">
<span class="sig-name descname"><span class="pre">compute_encodings</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/nn/base.html#BaseQuantizationMixin.compute_encodings"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#aimet_torch.v2.nn.base.BaseQuantizationMixin.compute_encodings" title="Permalink to this definition"></a></dt>
<dd><p>Enters the <cite>compute_encodings</cite> context for all <cite>QuantizerBase</cite> objects in the layer. Inside this context,
each quantizer will observe all inputs passed to the quantizer and will compute quantization encodings upon
exiting the context.
Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">QuantizedLinear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Quantize</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">qlinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.base.BaseQuantizationMixin.from_module">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/nn/base.html#BaseQuantizationMixin.from_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#aimet_torch.v2.nn.base.BaseQuantizationMixin.from_module" title="Permalink to this definition"></a></dt>
<dd><p>Create an instance of quantized module from a regular module instance. The resulting quantized module contains
the same attributes and parameters as the original module, but may be assigned input, output and parameter
quantizers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>module</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code>) – Floating point module to quantize</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Quantized version of the original module</p>
</dd>
</dl>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantized_linear</span> <span class="o">=</span> <span class="n">FakeQuantizationMixin</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span><span class="n">linear</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">quantized_linear</span><span class="o">.</span><span class="n">weight</span> <span class="ow">is</span> <span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">quantized_linear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">)</span>
<span class="go">ModuleDict(</span>
<span class="go">    (weight): None</span>
<span class="go">    (bias): None</span>
<span class="go">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.base.BaseQuantizationMixin.get_original_module">
<span class="sig-name descname"><span class="pre">get_original_module</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/nn/base.html#BaseQuantizationMixin.get_original_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#aimet_torch.v2.nn.base.BaseQuantizationMixin.get_original_module" title="Permalink to this definition"></a></dt>
<dd><p>Returns the floating point version of quantized module</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.base.BaseQuantizationMixin.quantized_forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">quantized_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/nn/base.html#BaseQuantizationMixin.quantized_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#aimet_torch.v2.nn.base.BaseQuantizationMixin.quantized_forward" title="Permalink to this definition"></a></dt>
<dd><p>Forward function for quantized module.
This method will replace the original forward function.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this heading"></a></h2>
<p>The quantization behavior of a quantized module is controlled by the <a class="reference internal" href="quantizer.html#api-torch-quantizers"><span class="std std-ref">quantizers</span></a> contained within the input, output,
and parameter quantizer attributes listed below.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 21%" />
<col style="width: 27%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Attribute</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>input_quantizers</p></td>
<td><p>torch.nn.ModuleList</p></td>
<td><p>List of quantizers for input tensors</p></td>
</tr>
<tr class="row-odd"><td><p>param_quantizers</p></td>
<td><p>torch.nn.ModuleDict</p></td>
<td><p>Dict mapping parameter names to quantizers</p></td>
</tr>
<tr class="row-even"><td><p>output_quantizers</p></td>
<td><p>torch.nn.ModuleList</p></td>
<td><p>List of quantizers for output tensors</p></td>
</tr>
</tbody>
</table>
<p>By assigning and configuring <a class="reference internal" href="quantizer.html#api-torch-quantizers"><span class="std std-ref">quantizers</span></a> to these structures, we define the type of quantization applied to the corresponding
input index, output index, or parameter name. By default, all the quantizers are set to <cite>None</cite>, meaning that no quantization
will be applied to the respective tensor.</p>
<dl>
<dt>Example: Create a linear layer which performs only per-channel weight quantization</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">aimet_torch.v2.quantization</span> <span class="kn">import</span> <span class="n">affine</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">aimet_torch</span><span class="o">.</span><span class="n">v2</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">QuantizedLinear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Per-channel weight quantization is performed over the `out_features` dimension, so encodings are shape (10, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_channel_quantizer</span> <span class="o">=</span> <span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">per_channel_quantizer</span>
</pre></div>
</div>
</dd>
<dt>Example: Create an elementwise multiply layer which quantizes only the output and the second input</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qmul</span> <span class="o">=</span> <span class="n">aimet_torch</span><span class="o">.</span><span class="n">v2</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">QuantizedMultiply</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qmul</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qmul</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p>In some cases, it may make sense for multiple tensors to share the same quantizer. In this case, we can assign the same
quantizer to multiple indices.</p>
<dl>
<dt>Example: Create an elementwise add layer which shares the same quantizer between its inputs</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qadd</span> <span class="o">=</span> <span class="n">aimet_torch</span><span class="o">.</span><span class="n">v2</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">QuantizedAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantizer</span> <span class="o">=</span> <span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qadd</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">quantizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qadd</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">quantizer</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="section" id="computing-encodings">
<h2>Computing Encodings<a class="headerlink" href="#computing-encodings" title="Permalink to this heading"></a></h2>
<p>Before a module can compute a quantized forward pass, all quantizers must first be calibrated inside a <cite>compute_encodings</cite>
context. When a quantized module enters the <cite>compute_encodings</cite> context, it first disables all input and output quantization
while the quantizers observe the statistics of the activation tensors passing through them. Upon exiting the context,
the quantizers calculate appropriate quantization encodings based on these statistics (exactly <em>how</em> the encodings are
computed is determined by each quantizer’s <a class="reference internal" href="encoding_analyzer.html#api-torch-encoding-analyzer"><span class="std std-ref">encoding analyzer</span></a>).</p>
<dl>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">aimet_torch</span><span class="o">.</span><span class="n">v2</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">QuantizedLinear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">():</span>
<span class="gp">... </span>    <span class="c1"># Pass several samples through the layer to ensure representative statistics</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">calibration_data_loader</span><span class="p">:</span>
<span class="gp">... </span>        <span class="n">qlinear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd>
</dl>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../user_guide/adaround.html" class="btn btn-neutral float-left" title="AIMET AdaRound" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="quantizer.html" class="btn btn-neutral float-right" title="Quantizers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Qualcomm Innovation Center, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>