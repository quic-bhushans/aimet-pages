<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quantized Modules &mdash; AI Model Efficiency Toolkit Documentation: ver 1.35.1</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../_static/style.css" />
      <link rel="stylesheet" href="../_static/style.css" type="text/css" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Quantizers" href="quantizer.html" />
    <link rel="prev" title="AIMET AdaRound" href="../user_guide/adaround.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

    
    
    <a href="index.html" class="icon icon-home">
    AI Model Efficiency Toolkit
      <img src="../_static/brain_logo.png" class="logo" alt="Logo"/>
    </a>
      <div class="version">
        1.35.1
      </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install/install_host.html">Install in Host Machine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install/install_docker.html">Install in Docker Container</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="tutorials/quickstart_guide.html">Quickstart Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/ptq.html">Post-Training Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Feature Descriptions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/adaround.html"> Adaptive Rounding (AdaRound)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">AIMET PyTorch API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quantized Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantizer.html">Quantizers</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/nn.quantization_mixin.html">QuantizationMixin</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/quantization/affine/index.html">quantization.affine</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/quantization/float/index.html">quantization.float</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoding_analyzer.html">Encoding Analyzers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">AI Model Efficiency Toolkit</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Quantized Modules</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/torch_docs/quantized_modules.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul><div class="rst-breadcrumbs-buttons" role="navigation" aria-label="Sequential page navigation">
        <a href="../user_guide/adaround.html" class="btn btn-neutral float-left" title="AIMET AdaRound" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="quantizer.html" class="btn btn-neutral float-right" title="Quantizers" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
  </div>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quantized-modules">
<span id="api-torch-quantized-modules"></span><h1>Quantized Modules<a class="headerlink" href="#quantized-modules" title="Permalink to this heading"></a></h1>
<p>To simulate the effects of running networks at a reduced bitwidth, AIMET introduced <cite>quantized modules</cite>, the extension of
standard torch.nn.Modules with some extra capabilities for quantization.
These quantized modules serve as drop-in replacements for their PyTorch counterparts, but can
hold input, output, and parameter <a class="reference internal" href="quantizer.html#api-torch-quantizers"><span class="std std-ref">quantizers</span></a> to perform quantization operations during the
module’s forward pass and compute quantization encodings.</p>
<p>More specifically, a quantized module inherits both from <a class="reference internal" href="#api-torch-quantization-mixin-summary"><span class="std std-ref">QuantizationMixin</span></a> and a native torch.nn.Module type,
typically with “Quantized-” prefix prepended to the original class name, such as QuantizedConv2d for torch.nn.Conv2d or QuantizedSoftmax for torch.nn.Softmax.
For more detailed API reference of QuantizationMixin class, see <a class="reference internal" href="api/nn.quantization_mixin.html#api-torch-quantization-mixin"><span class="std std-ref">QuantizationMixin API reference</span></a>.
For the full list of all built-in quantized modules in AIMET, see <a class="reference internal" href="#api-quantized-module-class-table"><span class="std std-ref">Quantized Module Classes</span></a></p>
<section id="top-level-api">
<h2>Top-level API<a class="headerlink" href="#top-level-api" title="Permalink to this heading"></a></h2>
<span class="target" id="api-torch-quantization-mixin-summary"></span><dl class="py class">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.QuantizationMixin">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">aimet_torch.v2.nn.</span></span><span class="sig-name descname"><span class="pre">QuantizationMixin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/nn/true_quant.html#QuantizationMixin"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#aimet_torch.v2.nn.QuantizationMixin" title="Permalink to this definition"></a></dt>
<dd><p>Mixin that adds quantization functionality on top of regular pytorch modules.</p>
<p>Specifically, a quantized module will quantize input, output, and parameter tensors with
its held <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code> objects during the <a class="reference internal" href="#aimet_torch.v2.nn.QuantizationMixin.forward" title="aimet_torch.v2.nn.QuantizationMixin.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method and use the inherited <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>
forward method to compute the layer operation. If all input, output, and parameter quantizers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, a
quantized module will behave exactly the same as its parent <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.QuantizationMixin.input_quantizers">
<span class="sig-name descname"><span class="pre">input_quantizers</span></span><a class="headerlink" href="#aimet_torch.v2.nn.QuantizationMixin.input_quantizers" title="Permalink to this definition"></a></dt>
<dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code> containing <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code> objects to be applied
to the layer’s input tensors</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.QuantizationMixin.output_quantizers">
<span class="sig-name descname"><span class="pre">output_quantizers</span></span><a class="headerlink" href="#aimet_torch.v2.nn.QuantizationMixin.output_quantizers" title="Permalink to this definition"></a></dt>
<dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ModuleList</span></code> containing <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code> objects to be applied
to the layer’s output tensors</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.QuantizationMixin.param_quantizers">
<span class="sig-name descname"><span class="pre">param_quantizers</span></span><a class="headerlink" href="#aimet_torch.v2.nn.QuantizationMixin.param_quantizers" title="Permalink to this definition"></a></dt>
<dd><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.ModuleDict</span></code> mapping parameter names to associated <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code>
objects</p>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">QuantizedLinear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="p">)</span>
<span class="go">QuantizedLinear(</span>
<span class="go">  in_features=10, out_features=10, bias=True</span>
<span class="go">  (param_quantizers): ModuleDict(</span>
<span class="go">    (weight): None</span>
<span class="go">    (bias): None</span>
<span class="go">  )</span>
<span class="go">  (input_quantizers): ModuleList(</span>
<span class="go">    (0): None</span>
<span class="go">  )</span>
<span class="go">  (output_quantizers): ModuleList(</span>
<span class="go">    (0): None</span>
<span class="go">  )</span>
<span class="go">)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.QuantizationMixin.__quant_init__">
<span class="sig-name descname"><span class="pre">__quant_init__</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#aimet_torch.v2.nn.QuantizationMixin.__quant_init__" title="Permalink to this definition"></a></dt>
<dd><p>Initializer for quantized module. This method will be invoked right after <code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code>.</p>
<p>This method initializes the <a class="reference internal" href="#aimet_torch.v2.nn.QuantizationMixin.input_quantizers" title="aimet_torch.v2.nn.QuantizationMixin.input_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">input_quantizers</span></code></a>, <a class="reference internal" href="#aimet_torch.v2.nn.QuantizationMixin.output_quantizers" title="aimet_torch.v2.nn.QuantizationMixin.output_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_quantizers</span></code></a>, and <a class="reference internal" href="#aimet_torch.v2.nn.QuantizationMixin.param_quantizers" title="aimet_torch.v2.nn.QuantizationMixin.param_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">param_quantizers</span></code></a>
structures to the appropriate sizes based on the number of input tensors, output tensors, and parameters of the
base <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Module</span></code> class. All quantizers are initializd to <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>For custom quantized classes, this method should be overridden to set the appropriate lengths of
<a class="reference internal" href="#aimet_torch.v2.nn.QuantizationMixin.input_quantizers" title="aimet_torch.v2.nn.QuantizationMixin.input_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">input_quantizers</span></code></a> and <a class="reference internal" href="#aimet_torch.v2.nn.QuantizationMixin.output_quantizers" title="aimet_torch.v2.nn.QuantizationMixin.output_quantizers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">output_quantizers</span></code></a> for the given base class.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.QuantizationMixin.compute_encodings">
<span class="sig-name descname"><span class="pre">compute_encodings</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/nn/true_quant.html#QuantizationMixin.compute_encodings"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#aimet_torch.v2.nn.QuantizationMixin.compute_encodings" title="Permalink to this definition"></a></dt>
<dd><p>Enters the <a class="reference internal" href="#aimet_torch.v2.nn.QuantizationMixin.compute_encodings" title="aimet_torch.v2.nn.QuantizationMixin.compute_encodings"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_encodings()</span></code></a> context for all <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerBase</span></code> objects in the layer.</p>
<p>Inside this context, each quantizer will observe all inputs passed to the quantizer and will compute
quantization encodings upon exiting the context.</p>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">QuantizedLinear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Quantize</span><span class="p">((),</span> <span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">qlinear</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="aimet_torch.v2.nn.QuantizationMixin.forward">
<em class="property"><span class="pre">abstract</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/aimet_torch/v2/nn/true_quant.html#QuantizationMixin.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#aimet_torch.v2.nn.QuantizationMixin.forward" title="Permalink to this definition"></a></dt>
<dd><p>Computes a quantized version of the parent module’s forward method.</p>
<p>The <a class="reference internal" href="#aimet_torch.v2.nn.QuantizationMixin.forward" title="aimet_torch.v2.nn.QuantizationMixin.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> method should perform the following logic in order:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Apply existing input quantizers to input tensors</p></li>
<li><p>Apply existing param quantizers to the layer’s parameters</p></li>
<li><p>Call the inherited <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> forward method with quantized inputs and parameters</p></li>
<li><p>Apply existing output quantizers to the outputs of the forward method</p></li>
</ol>
</div></blockquote>
<p>If all input, output, and parameter quantizers are <code class="docutils literal notranslate"><span class="pre">None</span></code>, this method will behave exactly the same as
its parent module’s forward pass.</p>
</dd></dl>

</dd></dl>

</section>
<section id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this heading"></a></h2>
<p>The quantization behavior of a quantized module is controlled by the <a class="reference internal" href="quantizer.html#api-torch-quantizers"><span class="std std-ref">quantizers</span></a> contained within the input, output,
and parameter quantizer attributes listed below.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Attribute</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>input_quantizers</p></td>
<td><p>torch.nn.ModuleList</p></td>
<td><p>List of quantizers for input tensors</p></td>
</tr>
<tr class="row-odd"><td><p>param_quantizers</p></td>
<td><p>torch.nn.ModuleDict</p></td>
<td><p>Dict mapping parameter names to quantizers</p></td>
</tr>
<tr class="row-even"><td><p>output_quantizers</p></td>
<td><p>torch.nn.ModuleList</p></td>
<td><p>List of quantizers for output tensors</p></td>
</tr>
</tbody>
</table>
<p>By assigning and configuring <a class="reference internal" href="quantizer.html#api-torch-quantizers"><span class="std std-ref">quantizers</span></a> to these structures, we define the type of quantization applied to the corresponding
input index, output index, or parameter name. By default, all the quantizers are set to <cite>None</cite>, meaning that no quantization
will be applied to the respective tensor.</p>
<dl>
<dt>Example: Create a linear layer which performs only per-channel weight quantization</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">aimet_torch.v2</span> <span class="k">as</span> <span class="nn">aimet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">aimet_torch.quantization</span> <span class="k">as</span> <span class="nn">Q</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">aimet</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">QuantizedLinear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Per-channel weight quantization is performed over the `out_features` dimension, so encodings are shape (10, 1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">per_channel_quantizer</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">per_channel_quantizer</span>
</pre></div>
</div>
</dd>
<dt>Example: Create an elementwise multiply layer which quantizes only the output and the second input</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qmul</span> <span class="o">=</span> <span class="n">aimet</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">custom</span><span class="o">.</span><span class="n">QuantizedMultiply</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qmul</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qmul</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p>In some cases, it may make sense for multiple tensors to share the same quantizer. In this case, we can assign the same
quantizer to multiple indices.</p>
<dl>
<dt>Example: Create an elementwise add layer which shares the same quantizer between its inputs</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qadd</span> <span class="o">=</span> <span class="n">aimet</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">custom</span><span class="o">.</span><span class="n">QuantizedAdd</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">quantizer</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qadd</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">quantizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qadd</span><span class="o">.</span><span class="n">input_quantizers</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">quantizer</span>
</pre></div>
</div>
</dd>
</dl>
</section>
<section id="computing-encodings">
<h2>Computing Encodings<a class="headerlink" href="#computing-encodings" title="Permalink to this heading"></a></h2>
<p>Before a module can compute a quantized forward pass, all quantizers must first be calibrated inside a <cite>compute_encodings</cite>
context. When a quantized module enters the <cite>compute_encodings</cite> context, it first disables all input and output quantization
while the quantizers observe the statistics of the activation tensors passing through them. Upon exiting the context,
the quantizers calculate appropriate quantization encodings based on these statistics (exactly <em>how</em> the encodings are
computed is determined by each quantizer’s <a class="reference internal" href="encoding_analyzer.html#api-torch-encoding-analyzer"><span class="std std-ref">encoding analyzer</span></a>).</p>
<dl>
<dt>Example:</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span> <span class="o">=</span> <span class="n">aimet</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">QuantizedLinear</span><span class="p">(</span><span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">in_features</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">affine</span><span class="o">.</span><span class="n">QuantizeDequantize</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bitwidth</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">qlinear</span><span class="o">.</span><span class="n">compute_encodings</span><span class="p">():</span>
<span class="gp">... </span>    <span class="c1"># Pass several samples through the layer to ensure representative statistics</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">calibration_data_loader</span><span class="p">:</span>
<span class="gp">... </span>        <span class="n">qlinear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="o">.</span><span class="n">output_quantizers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">())</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">qlinear</span><span class="o">.</span><span class="n">param_quantizers</span><span class="p">[</span><span class="s2">&quot;weight&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">is_initialized</span><span class="p">())</span>
<span class="go">True</span>
</pre></div>
</div>
</dd>
</dl>
</section>
<section id="quantized-module-classes">
<span id="api-quantized-module-class-table"></span><h2>Quantized Module Classes<a class="headerlink" href="#quantized-module-classes" title="Permalink to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>nn.Module</p></th>
<th class="head"><p>QuantizationMixin</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>torch.nn.AdaptiveAvgPool1d</p></td>
<td><p>QuantizedAdaptiveAvgPool1d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.AdaptiveAvgPool2d</p></td>
<td><p>QuantizedAdaptiveAvgPool2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.AdaptiveAvgPool3d</p></td>
<td><p>QuantizedAdaptiveAvgPool3d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.AdaptiveMaxPool1d</p></td>
<td><p>QuantizedAdaptiveMaxPool1d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.AdaptiveMaxPool2d</p></td>
<td><p>QuantizedAdaptiveMaxPool2d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.AdaptiveMaxPool3d</p></td>
<td><p>QuantizedAdaptiveMaxPool3d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.AlphaDropout</p></td>
<td><p>QuantizedAlphaDropout</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.AvgPool1d</p></td>
<td><p>QuantizedAvgPool1d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.AvgPool2d</p></td>
<td><p>QuantizedAvgPool2d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.AvgPool3d</p></td>
<td><p>QuantizedAvgPool3d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.BatchNorm1d</p></td>
<td><p>QuantizedBatchNorm1d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.BatchNorm2d</p></td>
<td><p>QuantizedBatchNorm2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.BatchNorm3d</p></td>
<td><p>QuantizedBatchNorm3d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.CELU</p></td>
<td><p>QuantizedCELU</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.ChannelShuffle</p></td>
<td><p>QuantizedChannelShuffle</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.ConstantPad1d</p></td>
<td><p>QuantizedConstantPad1d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.ConstantPad2d</p></td>
<td><p>QuantizedConstantPad2d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.ConstantPad3d</p></td>
<td><p>QuantizedConstantPad3d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Conv1d</p></td>
<td><p>QuantizedConv1d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Conv2d</p></td>
<td><p>QuantizedConv2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Conv3d</p></td>
<td><p>QuantizedConv3d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.ConvTranspose1d</p></td>
<td><p>QuantizedConvTranspose1d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.ConvTranspose2d</p></td>
<td><p>QuantizedConvTranspose2d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.ConvTranspose3d</p></td>
<td><p>QuantizedConvTranspose3d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Dropout</p></td>
<td><p>QuantizedDropout</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Dropout2d</p></td>
<td><p>QuantizedDropout2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Dropout3d</p></td>
<td><p>QuantizedDropout3d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.ELU</p></td>
<td><p>QuantizedELU</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.FeatureAlphaDropout</p></td>
<td><p>QuantizedFeatureAlphaDropout</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Flatten</p></td>
<td><p>QuantizedFlatten</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Fold</p></td>
<td><p>QuantizedFold</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.FractionalMaxPool2d</p></td>
<td><p>QuantizedFractionalMaxPool2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.FractionalMaxPool3d</p></td>
<td><p>QuantizedFractionalMaxPool3d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.GELU</p></td>
<td><p>QuantizedGELU</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.GLU</p></td>
<td><p>QuantizedGLU</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.GroupNorm</p></td>
<td><p>QuantizedGroupNorm</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Hardshrink</p></td>
<td><p>QuantizedHardshrink</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Hardsigmoid</p></td>
<td><p>QuantizedHardsigmoid</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Hardswish</p></td>
<td><p>QuantizedHardswish</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Hardtanh</p></td>
<td><p>QuantizedHardtanh</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.InstanceNorm1d</p></td>
<td><p>QuantizedInstanceNorm1d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.InstanceNorm2d</p></td>
<td><p>QuantizedInstanceNorm2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.InstanceNorm3d</p></td>
<td><p>QuantizedInstanceNorm3d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.LPPool1d</p></td>
<td><p>QuantizedLPPool1d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.LPPool2d</p></td>
<td><p>QuantizedLPPool2d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.LayerNorm</p></td>
<td><p>QuantizedLayerNorm</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.LeakyReLU</p></td>
<td><p>QuantizedLeakyReLU</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Linear</p></td>
<td><p>QuantizedLinear</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.LocalResponseNorm</p></td>
<td><p>QuantizedLocalResponseNorm</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.LogSigmoid</p></td>
<td><p>QuantizedLogSigmoid</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.LogSoftmax</p></td>
<td><p>QuantizedLogSoftmax</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.MaxPool1d</p></td>
<td><p>QuantizedMaxPool1d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.MaxPool2d</p></td>
<td><p>QuantizedMaxPool2d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.MaxPool3d</p></td>
<td><p>QuantizedMaxPool3d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.MaxUnpool1d</p></td>
<td><p>QuantizedMaxUnpool1d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.MaxUnpool2d</p></td>
<td><p>QuantizedMaxUnpool2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.MaxUnpool3d</p></td>
<td><p>QuantizedMaxUnpool3d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Mish</p></td>
<td><p>QuantizedMish</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.PReLU</p></td>
<td><p>QuantizedPReLU</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.PixelShuffle</p></td>
<td><p>QuantizedPixelShuffle</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.PixelUnshuffle</p></td>
<td><p>QuantizedPixelUnshuffle</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.RReLU</p></td>
<td><p>QuantizedRReLU</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.ReLU</p></td>
<td><p>QuantizedReLU</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.ReLU6</p></td>
<td><p>QuantizedReLU6</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.ReflectionPad1d</p></td>
<td><p>QuantizedReflectionPad1d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.ReflectionPad2d</p></td>
<td><p>QuantizedReflectionPad2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.ReplicationPad1d</p></td>
<td><p>QuantizedReplicationPad1d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.ReplicationPad2d</p></td>
<td><p>QuantizedReplicationPad2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.ReplicationPad3d</p></td>
<td><p>QuantizedReplicationPad3d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.SELU</p></td>
<td><p>QuantizedSELU</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.SiLU</p></td>
<td><p>QuantizedSiLU</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Sigmoid</p></td>
<td><p>QuantizedSigmoid</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Softmax</p></td>
<td><p>QuantizedSoftmax</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Softmax2d</p></td>
<td><p>QuantizedSoftmax2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Softmin</p></td>
<td><p>QuantizedSoftmin</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Softplus</p></td>
<td><p>QuantizedSoftplus</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Softshrink</p></td>
<td><p>QuantizedSoftshrink</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Softsign</p></td>
<td><p>QuantizedSoftsign</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Tanh</p></td>
<td><p>QuantizedTanh</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Tanhshrink</p></td>
<td><p>QuantizedTanhshrink</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Threshold</p></td>
<td><p>QuantizedThreshold</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Unflatten</p></td>
<td><p>QuantizedUnflatten</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Unfold</p></td>
<td><p>QuantizedUnfold</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Upsample</p></td>
<td><p>QuantizedUpsample</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.UpsamplingBilinear2d</p></td>
<td><p>QuantizedUpsamplingBilinear2d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.UpsamplingNearest2d</p></td>
<td><p>QuantizedUpsamplingNearest2d</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.ZeroPad2d</p></td>
<td><p>QuantizedZeroPad2d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.BCELoss</p></td>
<td><p>QuantizedBCELoss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.BCEWithLogitsLoss</p></td>
<td><p>QuantizedBCEWithLogitsLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.Bilinear</p></td>
<td><p>QuantizedBilinear</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.CTCLoss</p></td>
<td><p>QuantizedCTCLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.CosineSimilarity</p></td>
<td><p>QuantizedCosineSimilarity</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.CrossEntropyLoss</p></td>
<td><p>QuantizedCrossEntropyLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.HingeEmbeddingLoss</p></td>
<td><p>QuantizedHingeEmbeddingLoss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.HuberLoss</p></td>
<td><p>QuantizedHuberLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.KLDivLoss</p></td>
<td><p>QuantizedKLDivLoss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.L1Loss</p></td>
<td><p>QuantizedL1Loss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.MSELoss</p></td>
<td><p>QuantizedMSELoss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.MultiLabelMarginLoss</p></td>
<td><p>QuantizedMultiLabelMarginLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.MultiLabelSoftMarginLoss</p></td>
<td><p>QuantizedMultiLabelSoftMarginLoss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.MultiMarginLoss</p></td>
<td><p>QuantizedMultiMarginLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.NLLLoss</p></td>
<td><p>QuantizedNLLLoss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.NLLLoss2d</p></td>
<td><p>QuantizedNLLLoss2d</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.PairwiseDistance</p></td>
<td><p>QuantizedPairwiseDistance</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.PoissonNLLLoss</p></td>
<td><p>QuantizedPoissonNLLLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.SmoothL1Loss</p></td>
<td><p>QuantizedSmoothL1Loss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.SoftMarginLoss</p></td>
<td><p>QuantizedSoftMarginLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.CosineEmbeddingLoss</p></td>
<td><p>QuantizedCosineEmbeddingLoss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.GaussianNLLLoss</p></td>
<td><p>QuantizedGaussianNLLLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.MarginRankingLoss</p></td>
<td><p>QuantizedMarginRankingLoss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.TripletMarginLoss</p></td>
<td><p>QuantizedTripletMarginLoss</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.TripletMarginWithDistanceLoss</p></td>
<td><p>QuantizedTripletMarginWithDistanceLoss</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.Embedding</p></td>
<td><p>QuantizedEmbedding</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.EmbeddingBag</p></td>
<td><p>QuantizedEmbeddingBag</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.GRU</p></td>
<td><p>QuantizedGRU</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.RNN</p></td>
<td><p>QuantizedRNN</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.GRUCell</p></td>
<td><p>QuantizedGRUCell</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.RNNCell</p></td>
<td><p>QuantizedRNNCell</p></td>
</tr>
<tr class="row-even"><td><p>torch.nn.LSTM</p></td>
<td><p>QuantizedLSTM</p></td>
</tr>
<tr class="row-odd"><td><p>torch.nn.LSTMCell</p></td>
<td><p>QuantizedLSTMCell</p></td>
</tr>
<tr class="row-even"><td><p>aimet_torch.v2.nn.custom.AvgPool2d</p></td>
<td><p>QuantizedAvgPool2d</p></td>
</tr>
<tr class="row-odd"><td><p>aimet_torch.v2.nn.custom.CumSum</p></td>
<td><p>QuantizedCumSum</p></td>
</tr>
<tr class="row-even"><td><p>aimet_torch.v2.nn.custom.Sin</p></td>
<td><p>QuantizedSin</p></td>
</tr>
<tr class="row-odd"><td><p>aimet_torch.v2.nn.custom.Cos</p></td>
<td><p>QuantizedCos</p></td>
</tr>
<tr class="row-even"><td><p>aimet_torch.v2.nn.custom.RSqrt</p></td>
<td><p>QuantizedRSqrt</p></td>
</tr>
<tr class="row-odd"><td><p>aimet_torch.v2.nn.custom.Reshape</p></td>
<td><p>QuantizedReshape</p></td>
</tr>
<tr class="row-even"><td><p>aimet_torch.v2.nn.custom.MatMul</p></td>
<td><p>QuantizedMatMul</p></td>
</tr>
<tr class="row-odd"><td><p>aimet_torch.v2.nn.custom.Add</p></td>
<td><p>QuantizedAdd</p></td>
</tr>
<tr class="row-even"><td><p>aimet_torch.v2.nn.custom.Multiply</p></td>
<td><p>QuantizedMultiply</p></td>
</tr>
<tr class="row-odd"><td><p>aimet_torch.v2.nn.custom.Subtract</p></td>
<td><p>QuantizedSubtract</p></td>
</tr>
<tr class="row-even"><td><p>aimet_torch.v2.nn.custom.Divide</p></td>
<td><p>QuantizedDivide</p></td>
</tr>
<tr class="row-odd"><td><p>aimet_torch.v2.nn.custom.Bmm</p></td>
<td><p>QuantizedBmm</p></td>
</tr>
<tr class="row-even"><td><p>aimet_torch.v2.nn.custom.Baddbmm</p></td>
<td><p>QuantizedBaddbmm</p></td>
</tr>
<tr class="row-odd"><td><p>aimet_torch.v2.nn.custom.Addmm</p></td>
<td><p>QuantizedAddmm</p></td>
</tr>
<tr class="row-even"><td><p>aimet_torch.v2.nn.custom.Concat</p></td>
<td><p>QuantizedConcat</p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../user_guide/adaround.html" class="btn btn-neutral float-left" title="AIMET AdaRound" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="quantizer.html" class="btn btn-neutral float-right" title="Quantizers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Qualcomm Innovation Center, Inc..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>